{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","mount_file_id":"1EBl6mB4XL3NPdwtdlAP2HeFkmGMT9A_N","authorship_tag":"ABX9TyPBdW+oo1hRlN5QGAUYrqbM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"e0323948b2e84744a6d272097839119c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0b2e4954acbb42638d1ce2f45f647da9","IPY_MODEL_7a061258de8643b9a4413ab6821c9314","IPY_MODEL_4e5aee2b9cdb4576971e56431c8774cf"],"layout":"IPY_MODEL_b3dfae063ac94acc9c8aa1fcbf5bed31"}},"0b2e4954acbb42638d1ce2f45f647da9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_40261365f96b4417b16294fc66e48ff7","placeholder":"​","style":"IPY_MODEL_579f408a5e4443d48f428af09ec7a50d","value":"Loading checkpoint shards: 100%"}},"7a061258de8643b9a4413ab6821c9314":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_41110ff3e04b460892ef3adc7ac65702","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c10cf156101143eea5a925fff56d3834","value":3}},"4e5aee2b9cdb4576971e56431c8774cf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea3c2ef8bb284a16986e9534fcf9e3e7","placeholder":"​","style":"IPY_MODEL_fbee67b9cd104bf2b1445062c364b6c4","value":" 3/3 [00:35&lt;00:00, 11.44s/it]"}},"b3dfae063ac94acc9c8aa1fcbf5bed31":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40261365f96b4417b16294fc66e48ff7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"579f408a5e4443d48f428af09ec7a50d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"41110ff3e04b460892ef3adc7ac65702":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c10cf156101143eea5a925fff56d3834":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ea3c2ef8bb284a16986e9534fcf9e3e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fbee67b9cd104bf2b1445062c364b6c4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["%%capture\n","!pip install fire torch peft transformers"],"metadata":{"id":"JMAnBqRqOAwu"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QJsUojGsNyd8"},"outputs":[],"source":["import os\n","import sys\n","\n","import fire\n","import torch\n","\n","from peft import (\n","    PeftModel,\n","    PeftConfig,\n",")\n","\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n",")\n","\n","# 훈련가중치 최종 누적 경로를 얻기 위해\n","from transformers.trainer_utils import get_last_checkpoint"]},{"cell_type":"code","source":["base_model= \"/content/drive/MyDrive/ToyProject/for_Colab/Resource/polyglot-ko-1.3b\"         # 기반 모델 경로\n","peft_root = \"/content/drive/MyDrive/ToyProject/for_Colab/Train/lora/f16/\"                   # PEFT 훈련가중치 루트경로\n","output_dir: str = \"/content/drive/MyDrive/ToyProject/for_Colab/Resource/polyglot-ko-1.3b-loraf16\"   # 병합된 사전훈련모델 저장경로\n","max_shard_size : str = \"10GB\"  # 모델 파일 단위 사이즈\n","\n","# --------------------------------------\n","# 0. 준비 작업\n","# --------------------------------------\n","# 0-1. 기반모델 경로(base_path) 얻기\n","# ------------------\n","base_path = None\n","if base_model and os.path.isdir(base_model):\n","    base_path = base_model\n","if not base_path:\n","    # PEFT 가중치 경로로부터, 구성정보(adapter_config.json) 로드\n","    peft_config = PeftConfig.from_pretrained(peft_root)\n","    base_path = peft_config.base_model_name_or_path\n","# [base_path] 파라미터 체크\n","assert(os.path.isdir(base_path)), \"Found not a path for base model. Please specify a --base_model, e.g. --base_model='resources/polyglot-ko'\"\n","# ------------------\n","# 0-2. PEFT 루트경로(peft_root)로부터, 최종 체크포인트(훈련가중치) 경로(last_peft_path) 얻기\n","# ------------------\n","\n","\n","last_peft_path = get_last_checkpoint(peft_root)\n","if last_peft_path is None:\n","    if len(os.listdir(peft_root)) > 0:\n","        raise ValueError(f\"PEFT directory({peft_root}) already exists and is not empty.\")\n","    else:\n","        raise ValueError(f\"Found not checkpoint for PEFT in {peft_root}.\")\n","else:\n","    print(\n","        f\"Checkpoint detected: {last_peft_path}\\n\"\n","        \"----------------------------------------\"\n","    )"],"metadata":{"id":"HfXRcTINN9aV","executionInfo":{"status":"ok","timestamp":1723487858064,"user_tz":-540,"elapsed":8,"user":{"displayName":"장영(배뚱)","userId":"12533087895771299388"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b34245af-739f-4c62-9ffc-05f8dbf75faa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Checkpoint detected: /content/drive/MyDrive/ToyProject/for_Colab/Train/lora/f16/checkpoint-360\n","----------------------------------------\n"]}]},{"cell_type":"code","source":["# --------------------------------------\n","# 1. 기반 모델+토크나이저, PEFT 가중치 로드\n","# --------------------------------------\n","# 1-1. 사전훈련모델(komodel) 로드\n","# ------------------\n","print(\n","    f\"Loading model: {base_path}\\n\"\n","    \"----------------------------------------\"\n",")\n","komodel = AutoModelForCausalLM.from_pretrained(\n","    base_path,\n","    # load_in_8bit=False,\n","    torch_dtype=torch.float16,\n","    # device_map={\"\": \"cpu\"},\n",")\n","\"\"\"\n","komodel = AutoModelForCausalLM.from_pretrained(\n","    base_path,\n","    # 4비트 로드 양자화 적용\n","    use_safetensors=True,\n","    quantization_config=BitsAndBytesConfig(\n","        load_in_4bit=True,\n","        load_in_8bit=False,\n","        bnb_4bit_use_double_Quant=False,\n","        bnb_4bit_compute_dtype=torch.bfloat16,\n","        bnb_4bit_quant_type='nf4'\n","    ),\n","    torch_dtype=torch.float16,\n","    device_map=device_map,\n","    #trust_remote_code=True,        # HuggingFace 원격저장소에서 코드를 다운로드할 때, 코드를 신뢰할지 여부\n",")\n","\"\"\""],"metadata":{"id":"GvsnBdm_PpN5","executionInfo":{"status":"ok","timestamp":1723487895349,"user_tz":-540,"elapsed":37289,"user":{"displayName":"장영(배뚱)","userId":"12533087895771299388"}},"colab":{"base_uri":"https://localhost:8080/","height":154,"referenced_widgets":["e0323948b2e84744a6d272097839119c","0b2e4954acbb42638d1ce2f45f647da9","7a061258de8643b9a4413ab6821c9314","4e5aee2b9cdb4576971e56431c8774cf","b3dfae063ac94acc9c8aa1fcbf5bed31","40261365f96b4417b16294fc66e48ff7","579f408a5e4443d48f428af09ec7a50d","41110ff3e04b460892ef3adc7ac65702","c10cf156101143eea5a925fff56d3834","ea3c2ef8bb284a16986e9534fcf9e3e7","fbee67b9cd104bf2b1445062c364b6c4"]},"outputId":"5e31333d-083e-4f2c-d3b4-f55be90bad06"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading model: /content/drive/MyDrive/ToyProject/for_Colab/Resource/polyglot-ko-1.3b\n","----------------------------------------\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0323948b2e84744a6d272097839119c"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["\"\\nkomodel = AutoModelForCausalLM.from_pretrained(\\n    base_path,\\n    # 4비트 로드 양자화 적용\\n    use_safetensors=True,\\n    quantization_config=BitsAndBytesConfig(\\n        load_in_4bit=True,\\n        load_in_8bit=False,\\n        bnb_4bit_use_double_Quant=False,\\n        bnb_4bit_compute_dtype=torch.bfloat16,\\n        bnb_4bit_quant_type='nf4'\\n    ),\\n    torch_dtype=torch.float16,\\n    device_map=device_map,\\n    #trust_remote_code=True,        # HuggingFace 원격저장소에서 코드를 다운로드할 때, 코드를 신뢰할지 여부\\n)\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# ------------------\n","# 1-2. 사전훈련된 토크나이저(tokenizer) 로드\n","# ------------------\n","print(\n","    f\"Loading tokenizer: {base_path}\\n\"\n","    \"----------------------------------------\"\n",")\n","tokenizer = AutoTokenizer.from_pretrained(base_path)\n","# ------------------\n","# 1-3. 사전훈련모델에 PEFT 가중치 로드\n","# ------------------\n","print(\n","    f\"Loading PEFT weights: {last_peft_path}\\n\"\n","    \"----------------------------------------\"\n",")\n","lora_model = PeftModel.from_pretrained(\n","    komodel,\n","    last_peft_path,\n","    # device_map={\"\": \"cpu\"},\n","    torch_dtype=torch.float16,\n",")\n","# --------------------------------------\n","# 2. 사전훈련모델에 PEFT 가중치를 병합한 후, 기존 PEFT 제거\n","# --------------------------------------\n","print(\n","    \"Merging and Unload...\\n\"\n","    \"----------------------------------------\"\n",")\n","merged_model = lora_model.merge_and_unload(progressbar=True, safe_merge=True)"],"metadata":{"id":"5VLssMOUPtMB","executionInfo":{"status":"ok","timestamp":1723488073508,"user_tz":-540,"elapsed":4204,"user":{"displayName":"장영(배뚱)","userId":"12533087895771299388"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a5ae0d31-4d62-43bd-a155-9c465b7ded82"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"output_type":"stream","name":"stdout","text":["Loading tokenizer: /content/drive/MyDrive/ToyProject/for_Colab/Resource/polyglot-ko-1.3b\n","----------------------------------------\n","Loading PEFT weights: /content/drive/MyDrive/ToyProject/for_Colab/Train/lora/f16/checkpoint-360\n","----------------------------------------\n","Merging and Unload...\n","----------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Unloading and merging model: 100%|██████████| 367/367 [00:01<00:00, 198.87it/s]\n"]}]},{"cell_type":"code","source":["# --------------------------------------\n","# 3. 병합모델  훈련 시작\n","# --------------------------------------\n","print(\n","    \"Training...\\n\"\n","    \"----------------------------------------\"\n",")\n","merged_model.train(False)\n","# ------------------------------------"],"metadata":{"id":"LyCmaVzqP4ah","executionInfo":{"status":"ok","timestamp":1723488076988,"user_tz":-540,"elapsed":370,"user":{"displayName":"장영(배뚱)","userId":"12533087895771299388"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e9ce587e-e031-4a3e-f214-92557882cd22"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training...\n","----------------------------------------\n"]},{"output_type":"execute_result","data":{"text/plain":["GPTNeoXForCausalLM(\n","  (gpt_neox): GPTNeoXModel(\n","    (embed_in): Embedding(30080, 2048)\n","    (emb_dropout): Dropout(p=0.0, inplace=False)\n","    (layers): ModuleList(\n","      (0-23): 24 x GPTNeoXLayer(\n","        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n","        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n","        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n","        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n","        (attention): GPTNeoXSdpaAttention(\n","          (rotary_emb): GPTNeoXRotaryEmbedding()\n","          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n","          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n","          (attention_dropout): Dropout(p=0.0, inplace=False)\n","        )\n","        (mlp): GPTNeoXMLP(\n","          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n","          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n","          (act): GELUActivation()\n","        )\n","      )\n","    )\n","    (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (embed_out): Linear(in_features=2048, out_features=30080, bias=False)\n",")"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["# --------------------------------------\n","# 4. 병합모델 상태사전 교정\n","# --------------------------------------\n","print(\n","    \"Correcting state_dict...\\n\"\n","    \"----------------------------------------\"\n",")\n","# 병합모델 상태사전 얻기\n","merged_model_sd = merged_model.state_dict()\n","# 병합모델 상태사전에서, lora 모델의 가중치를 제거시킨 상태사전을 만든다.\n","deloreanized_sd = {\n","    k.replace(\"base_model.model.\", \"\"): v\n","    for k, v in merged_model_sd.items()\n","    if \"lora\" not in k\n","}\n","# --------------------------------------\n","# 5. [병합모델 & 토크나이저] 저장\n","# --------------------------------------\n","# 5-1. 모델 저장\n","# ------------------\n","# 미세조정 가중치 저장\n","print(\n","    f\"Saving model: {output_dir}\\n\"\n","    \"----------------------------------------\"\n",")\n","komodel.save_pretrained(\n","    output_dir,\n","    state_dict=deloreanized_sd,\n","    max_shard_size=max_shard_size,\n",")\n","# ------------------\n","# 5-2. 토크나이저 저장\n","# ------------------\n","print(\n","    f\"Saving tokenizer: {output_dir}\\n\"\n","    \"----------------------------------------\"\n",")\n","tokenizer.save_pretrained(output_dir)\n","# --------------------------------------"],"metadata":{"id":"XoVRID43P5Wq","executionInfo":{"status":"ok","timestamp":1723488096560,"user_tz":-540,"elapsed":11125,"user":{"displayName":"장영(배뚱)","userId":"12533087895771299388"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ffb42642-8f8a-4fce-f9c1-d2ef086c1b56"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Correcting state_dict...\n","----------------------------------------\n","Saving model: /content/drive/MyDrive/ToyProject/for_Colab/Resource/polyglot-ko-1.3b-loraf16\n","----------------------------------------\n","Saving tokenizer: /content/drive/MyDrive/ToyProject/for_Colab/Resource/polyglot-ko-1.3b-loraf16\n","----------------------------------------\n"]},{"output_type":"execute_result","data":{"text/plain":["('/content/drive/MyDrive/ToyProject/for_Colab/Resource/polyglot-ko-1.3b-loraf16/tokenizer_config.json',\n"," '/content/drive/MyDrive/ToyProject/for_Colab/Resource/polyglot-ko-1.3b-loraf16/special_tokens_map.json',\n"," '/content/drive/MyDrive/ToyProject/for_Colab/Resource/polyglot-ko-1.3b-loraf16/tokenizer.json')"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":[],"metadata":{"id":"Vq_ufNIlZ49b"},"execution_count":null,"outputs":[]}]}